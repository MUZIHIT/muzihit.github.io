---
title: CADE-Detecting and Explaining Concept Drift Samples for Security Applications
date: 2024-07-09 11:11:47
cover: ../static/CADE/cover.jpg
categories:
    - 科研
tags:
    - 未知攻击检测
    - attack detect
---
发表会议：USENIXSec 2021

**代码地址**：[https://github.com/whyisyoung/CADE](https://github.com/whyisyoung/CADE)

## 引子


为了对抗概念漂移的问题，设计一个模型CADE来实现对漂移样本的检测以及分类。
![[Pasted image 20240709100037.png]](../static/CADE/CADE-1.png)

检测偏离现有类的漂移样本
提供解释来解释检测到的漂移。
漂移检测模块：负责监测输入原分类模型中的数据是否存在问题，即检测漂移数据
解释漂移模块：训练了一个基于距离的语义解释函数来对发现的漂移数据进行解释，并未是否需要对模型进行再训练提供一个参考。


由于概念漂移，部署基于机器学习的安全应用可能非常具有挑战性。无论是恶意软件分类、入侵检测，还是在线滥用检测，基于学习的模型都是在"封闭世界"假设下工作的，希望测试数据的分布与训练数据的分布大致匹配。然而，模型部署的环境通常是随时间动态变化的。这种变化既可能包括良性玩家的有机行为变化，也可能包括攻击者的恶意突变和适应。因此，测试数据分布正在从原始训练数据中转移，这可能会导致模型出现严重的故障。为了解决概念漂移问题，大多数基于学习的模型需要定期重新训练。然而，再训练往往需要标注大量的新样本(代价昂贵)。更重要的是，也很难确定何时应该重新训练模型。延迟再训练会使过时的模型容易受到新的攻击。我们设想，对抗概念漂移需要建立一个监控系统来检查输入数据流和训练数据(和/或当前分类器)之间的关系。高层思路如图1所示。当原始分类器在生产空间中工作时，另一个系统应该周期性地检查分类器的合格程度，以便对输入的数据样本进行决策。检测模块(∂)可以过滤正在远离训练空间的漂移样本。更重要的是，为了推理(例如,攻击者突变、有机行为改变、先前未知的系统漏洞等)漂移的原因，我们需要一种解释方法(∑)将检测决策与语义有意义的特征联系起来。这两个模块对于开放世界的恶意软件检测是非常重要的。
之前的工作已经探索了通过直接检查原始分类器的预测置信度来检测漂移样本。置信度得分较低，说明进入的样本为漂移样本。然而，这个置信度评分是在假设所有类别已知(封闭世界)的基础上计算得到的概率(总和达到1.0)。一个不属于任何已有类的漂移样本可能被分配到一个高置信度的错误类(得到了已有工作的验证)中。最近的一项工作提出了计算传入样本与每个现有类之间的非一致性度量来确定适应度的想法。这种不一致性度量是基于一个距离函数来计算的，以量化这种不一致性。

## 本文方法

在本文中，我们提出了一种新的方法来检测漂移样本，并结合一种新的方法来解释检测决策。总的来说，我们构建了一个名为CADE的系统，简称为"用于漂移检测和解释的对比自编码器"。关键的挑战是推导出一个有效的距离函数来度量样本的相异性。我们利用对比学习的思想，根据已有的标签，从已有的训练数据中学习距离函数，而不是任意挑选距离函数。给定原始分类器的训练数据(多个类别)，我们将训练样本映射到一个低维隐空间。通过对比样本学习映射函数，扩大不同c的样本之间的距离
为了解释一个漂移的样本，我们识别了一组小的重要特征，这些特征将这个样本与它最近的类区分开来。一个关键的观察是，传统的(监督的)解释方法并不能很好地解释。其启示在于，有监督的解释方法要求两类(漂移样本和已有类)都有足够的样本来估计其分布。然而，由于漂移样本位于训练分布之外的稀疏空间，这一要求很难满足。相反，我们发现基于距离变化，即导致漂移样本与其最近类之间距离变化最大的特征来推导解释更有效。

在多类分类设置中，主要存在两类概念漂移。A型：引入新类：漂移样本来自训练数据集中不存在的新类。因此，原先训练好的分类器无法对漂移样本进行分类；B型：类内进化：漂移样本仍来自已有的类，但其行为模式与训练数据集中存在明显差异。在这种情况下，原始分类器很容易在这些漂移样本上出错。在本文中，我们主要关注Type A概念漂移，即在多类环境中引入一个新类。以恶意软件分类为例，我们的目标是检测和解释来自先前看不到的恶意软件家族的漂移样本。从本质上讲，漂移样本是训练数据中所有现有类的分布外样本。

我们认为预测概率在我们的问题情境中不太可能有效。原因是这个概率反映了对现有类(例如,样本在A类中的拟合情况要好于B类)的相对适应度。如果样本来自一个全新的类(既不是A类或者B )，预测概率可能会被极大地误导。许多先前的研究(例如,将错误的类与高概率相关联)。从根本上说，预测概率仍然继承了分类器的"封闭世界假设"，因此不适合检测漂移样本。

与预测概率相比，一个更有前途的方向是直接评估样本对给定类的拟合度。其思想是，与评估样本在A类中是否比B类更好不同，我们评估该样本在A类中与A类中其他训练样本相比的拟合程度。例如，自动编码器可以根据重构误差来评估样本对给定分布的拟合程度。然而，作为一种无监督方法，自动编码器在忽略标签(见第4节)的情况下，很难学习到训练分布的准确表示。在最近的一项工作中，若尔达内等人引入了一个名为Transcend的系统。它将一种"不符合度量"定义为适应度评价。Transcend使用一个可信度p值来量化测试样本xxx与共享同一类别的训练样本的相似程度。p是该类别中的样本与xxx中的其他样本至少不相似的比例。虽然这种度量可以准确地指出漂移的样本，但这样的系统高度依赖于对"相异性"的良好定义。

在高层，我们首先使用对比学习来学习训练数据的压缩表示。对比学习的一个关键好处是，相比于无监督方法，如自编码器和主成分分析( Principal Component Analysis，PCA ) ，它可以利用现有的标签来获得更好的性能。这使得我们可以从训练数据中学习一个距离函数来检测漂移样本。对于解释模块，我们将描述一个基于距离的解释公式来应对上述挑战。

解释模块旨在识别驱动测试样本远离现有类的最重要特征。具体来说，给定一个漂移样本xt，以及它在训练集中的最近类yt，我们想找出一个小的特征集合，使得xt成为类yt的离群点。为了实现这一目标，一个本能的反应是将其转换为一个监督学习模型的解释问题，这是一个很好的研究领域。由于离群空间的高度稀疏性，我们发现很难移动一个漂移样本来跨越决策边界，从而无法得出有意义的解释。受此启发，我们设计了一种新的针对漂移检测的解释方法，该方法解释了漂移样本与类内样本之间的距离，而不是决策边界。
![[Pasted image 20240709104648.png]](../static/CADE/CADE-2.png)
我们提出了一种新的方法，通过解释距离(即图3中的红色箭头)来识别重要特征。与基于决策边界进行决策的有监督分类器不同，漂移检测模型根据样本到质心的距离进行决策。因此，我们的目标是找到一组有助于将漂移样本xt移动到最近的质心yt的原始特征。通过这样的设计，我们不再需要强迫xt跨越边界，这是难以实现的。相反，我们对原始特征进行扰动，观察潜在空间中的距离变化。要实现这一想法，首先需要设计一个特征扰动机制。现有的大多数扰动方法都是针对图像设计的，其特点是数值性。在我们的案例中，xxxt中的特征可以是数值型的，也可以是分类型的，因此直接应用现有的方法会产生定义不明确的特征值。为了确保扰动对数值特征和类别特征都有意义，我们提出用参考训练样本中对应特征的值替换其特征值来扰动xt。
![[Pasted image 20240709105344.png]](../static/CADE/CADE-3.png)
同时也有助于保证扰动后的样本向着质心的粗略方向移动。与之前一样，扰动是在原始特征空间中进行的，其中特征具有语义含义。
## 贡献
本文的主要贡献有三：
提出了CADE来补充现有的基于监督学习的安全应用，以对抗概念漂移。我们介绍了一种基于对比表示学习的有效方法来检测漂移样本。
我们说明了监督解释方法在解释离群样本方面的局限性，并针对这种情况介绍了一种基于距离的解释方法。
我们通过两个应用对所提出的方法进行了广泛的评估。我们对一家证券公司的初步测试表明，CADE是有效的。我们在此发布了CADE的代码1，以支持未来的研究。